{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from math import exp, sqrt, pi, log\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_english(data):\n",
    "\n",
    "    data_desc_title = data[['description', 'title']]\n",
    "\n",
    "    data_desc_title_ls = data_desc_title.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "    ##### tokenization, casefolding#####\n",
    "\n",
    "    import nltk\n",
    "\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    tokenized = []\n",
    "\n",
    "    for data in data_desc_title_ls:\n",
    "\n",
    "        tokenized_desc = word_tokenize(data[0].lower())\n",
    "\n",
    "        tokenized_title = word_tokenize(data[1].lower())\n",
    "\n",
    "        tokenized.append([tokenized_desc, tokenized_title])\n",
    "\n",
    "\n",
    "\n",
    "    ###### normalization, lemmatization,and removing punctuation marks########\n",
    "\n",
    "    import nltk\n",
    "\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    import nltk\n",
    "\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "\n",
    "                    \"N\": wordnet.NOUN,\n",
    "\n",
    "                    \"V\": wordnet.VERB,\n",
    "\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "    tokenized_lemmatized = []\n",
    "\n",
    "    for data in tokenized:\n",
    "\n",
    "        lemmatized_tokenized_desc = []\n",
    "\n",
    "        for word in data[0]:\n",
    "\n",
    "            lemmatized_tokenized_desc.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "\n",
    "\n",
    "        lemmatized_tokenized_desc = [word for word in lemmatized_tokenized_desc if word.isalnum()]\n",
    "\n",
    "\n",
    "\n",
    "        lemmatized_tokenized_title = []\n",
    "\n",
    "        for word in data[1]:\n",
    "\n",
    "            lemmatized_tokenized_title.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "\n",
    "\n",
    "        lemmatized_tokenized_title = [word for word in lemmatized_tokenized_title if word.isalnum()]\n",
    "\n",
    "\n",
    "\n",
    "        tokenized_lemmatized.append([lemmatized_tokenized_desc, lemmatized_tokenized_title])\n",
    "\n",
    "    return tokenized_lemmatized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_stop_words_english(tokenized_lemmatized):\n",
    "\n",
    "    ######stop words#######\n",
    "\n",
    "    list_of_words_frequency = []\n",
    "\n",
    "    for data in tokenized_lemmatized:\n",
    "\n",
    "        for word in data[0]:\n",
    "\n",
    "            if (word not in [i[0] for i in list_of_words_frequency]):\n",
    "\n",
    "                list_of_words_frequency.append([word, 1])\n",
    "\n",
    "            else:\n",
    "\n",
    "                list_of_words_frequency[[i[0] for i in list_of_words_frequency].index(word)][1] += 1\n",
    "\n",
    "        for word in data[1]:\n",
    "\n",
    "            if (word not in [i[0] for i in list_of_words_frequency]):\n",
    "\n",
    "                list_of_words_frequency.append([word, 1])\n",
    "\n",
    "            else:\n",
    "\n",
    "                list_of_words_frequency[[i[0] for i in list_of_words_frequency].index(word)][1] += 1\n",
    "\n",
    "\n",
    "\n",
    "    list_of_words_frequency = sorted(list_of_words_frequency, key=lambda l: l[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "    number_of_stop_words = 10\n",
    "\n",
    "    stop_words = [i[0] for i in list_of_words_frequency][:number_of_stop_words]\n",
    "\n",
    "\n",
    "\n",
    "    with open('data/stop_words_english.txt', 'w', encoding='utf-8') as f:\n",
    "\n",
    "        for page in stop_words:\n",
    "\n",
    "            f.write(\"%s\\n\" % page)\n",
    "\n",
    "\n",
    "\n",
    "    tokenized_lemmatized_removed_stop_words = []\n",
    "\n",
    "    for data in tokenized_lemmatized:\n",
    "\n",
    "        lemmatized_tokenized_removed_stop_words_desc = [word for word in data[0] if word not in stop_words]\n",
    "\n",
    "\n",
    "\n",
    "        lemmatized_tokenized_removed_stop_words_title = [word for word in data[1] if word not in stop_words]\n",
    "\n",
    "\n",
    "\n",
    "        tokenized_lemmatized_removed_stop_words.append(\n",
    "\n",
    "            [lemmatized_tokenized_removed_stop_words_desc, lemmatized_tokenized_removed_stop_words_title])\n",
    "\n",
    "    return tokenized_lemmatized_removed_stop_words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_id_english(tokenized_lemmatized_removed_stop_words):\n",
    "\n",
    "    merged_id_english = []\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(tokenized_lemmatized_removed_stop_words)):\n",
    "\n",
    "        merged_id_english.append(\n",
    "\n",
    "            [i + 1, tokenized_lemmatized_removed_stop_words[i][0], tokenized_lemmatized_removed_stop_words[i][1]])\n",
    "\n",
    "    return merged_id_english\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preProcess(data,words):\n",
    "\n",
    "    if(words is not None):\n",
    "\n",
    "        data_desc_title = data[['description', 'title']]\n",
    "\n",
    "        data_desc_title_ls = data_desc_title.values.tolist()\n",
    "\n",
    "        tokenized_lemmatized = prepare_text_english(data)\n",
    "\n",
    "        tokenized_lemmatized_removed_stop_words = remove_stop_words_english(tokenized_lemmatized)\n",
    "\n",
    "        merged_id_english = add_id_english(tokenized_lemmatized_removed_stop_words)\n",
    "\n",
    "        dic={}\n",
    "\n",
    "        for i in range(len(merged_id_english)):\n",
    "\n",
    "            dic[data_desc_title_ls[i][0], data_desc_title_ls[i][1]] = merged_id_english[i][1], merged_id_english[i][2]\n",
    "\n",
    "        corpes = []\n",
    "\n",
    "        for i in tokenized_lemmatized_removed_stop_words:\n",
    "\n",
    "            corpes.extend(i[0])\n",
    "\n",
    "            corpes.extend(i[1])\n",
    "\n",
    "        corpes = set(corpes)\n",
    "\n",
    "        return dic, corpes\n",
    "\n",
    "    else:\n",
    "\n",
    "        res = []\n",
    "\n",
    "        for i in data_desc_title_ls:\n",
    "\n",
    "            decs = []\n",
    "\n",
    "            for j in i[0].split():\n",
    "\n",
    "                if (j in words):\n",
    "\n",
    "                    decs.append(j)\n",
    "\n",
    "\n",
    "\n",
    "            titles = []\n",
    "\n",
    "            for j in i[1].split():\n",
    "\n",
    "                if (j in words):\n",
    "\n",
    "                    titles.append(j)\n",
    "\n",
    "            res.append([decs, titles])\n",
    "\n",
    "        return res, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X(raw_data, words=None, coeff=2):\n",
    "    \n",
    "    main_data, words = preProcess(raw_data, words)\n",
    "    \n",
    "    n_doc = len(main_data)\n",
    "    n_word = len(words)\n",
    "    \n",
    "    vector_space_title = np.zeros((n_doc, n_word))\n",
    "    vector_space_text = np.zeros((n_doc, n_word))\n",
    "\n",
    "    for i in range(n_doc):\n",
    "        for y in main_data[i]['title']:\n",
    "            vector_space_title[i, words.index(y)] += 1\n",
    "        for y in main_data[i]['text']:\n",
    "            vector_space_text[i, words.index(y)] += 1\n",
    "\n",
    "    vector_space_title_idfs = np.log10(n_word / np.count_nonzero(vector_space_title, axis=0))\n",
    "    vector_space_text_idfs = np.log10(n_word / np.count_nonzero(vector_space_text, axis=0))\n",
    "    \n",
    "    vector_space_title_tfidfs = np.multiply(vector_space_title_tfs, vector_space_title_idfs)\n",
    "    vector_space_text_tfidfs = np.multiply(vector_space_text_tfs, vector_space_text_idfs)\n",
    "    \n",
    "    final_vector_space = coeff * vector_space_title_tfidfs + vector_space_text_tfidfs\n",
    "    \n",
    "    return final_vector_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_y(data):\n",
    "    data_views = data[['views']].values\n",
    "    labels = np.where(data_views >= np.median(data_views), 1, -1)\n",
    "    return lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'data_desc_title_ls' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b9dbd0054831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_phase1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/ted_talks.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_phase2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_phase2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_phase1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_phase1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-70af72c3ab93>\u001b[0m in \u001b[0;36mmake_X\u001b[0;34m(raw_data, words, coeff)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-de3685b6fd46>\u001b[0m in \u001b[0;36mpreProcess\u001b[0;34m(data, words)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_desc_title_ls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mdecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'data_desc_title_ls' referenced before assignment"
     ]
    }
   ],
   "source": [
    "train_data_phase2 = pd.read_csv(\"./data/train.csv\")\n",
    "test_data_phase2 = pd.read_csv(\"./data/test.csv\")\n",
    "data_phase1 = pd.read_csv(\"./data/ted_talks.csv\")\n",
    "\n",
    "X_train, words = make_X(train_data_phase2)\n",
    "X_test = make_X(test_data_phase2, words)\n",
    "X_phase1 = make_X(data_phase1, words)\n",
    "\n",
    "y_train = make_y(train_data_phase2)\n",
    "y_test = make_y(test_data_phase2)\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_predicted):\n",
    "    \n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    for y1, y2 in zip(y_true, y_predicted):\n",
    "        if y1 == y2:\n",
    "            if y1 == 1:\n",
    "                TP += 1\n",
    "            elif y2 == -1:\n",
    "                TN += 1\n",
    "        elif y1 != y2:\n",
    "            if y1 == 1:\n",
    "                FN += 1\n",
    "            elif y1 == -1:\n",
    "                FP += 1\n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    specificity = TN / (TN + FP)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    dic = {\"Precision\": precision, \"Recall\": recall, \"Accuracy\": accuracy, \"F1_Score\": f1,\n",
    "           \"Sensitivity\": recall, \"Specificity\": specificity}\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidianDistance(sample1, sample2):\n",
    "    dis = np.linalg.norm(sample1 - sample2)\n",
    "    return dis\n",
    "\n",
    "def euclidianDistance2(sample1, sample2):\n",
    "    distance = 0\n",
    "    for i in range(len(sample1)):\n",
    "        distance += pow(sample1[i] - sample2[i], 2)\n",
    "    distance = math.sqrt(distance)\n",
    "    return distance\n",
    "\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, k, dis):\n",
    "        self.k = k\n",
    "        self.dis = dis\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "\n",
    "    def distance(self, sample1, sample2, distance_type):\n",
    "        if distance_type == 'euclidian':\n",
    "            return euclidianDistance(sample1, sample2)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, test):\n",
    "\n",
    "        predict = []\n",
    "        \n",
    "        for x in test:\n",
    "\n",
    "            temp = []\n",
    "            for y in self.X:\n",
    "                temp += [self.distance(x, y, self.dis)]\n",
    "\n",
    "            temp = list(zip(temp, self.y))\n",
    "            temp = sorted(temp, key=lambda a_entry: a_entry[0])\n",
    "            temp = np.array(temp)\n",
    "            temp = temp[:self.k, 1]\n",
    "            temp = np.unique(temp, return_counts=True)\n",
    "            predict += [temp[0][temp[1].argmax()]]\n",
    "        \n",
    "        return np.array(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [1, 5, 9]\n",
    "\n",
    "best_acc1 = 0.0\n",
    "best_value = 0\n",
    "best_clf_knn = None\n",
    "\n",
    "for k in K:\n",
    "    \n",
    "    clf = KNN(k=k, dis='euclidian')\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_validation)\n",
    "    acc = evaluate(y_validation, predicted)[\"Accuracy\"]\n",
    "    \n",
    "    if acc >= best_acc1:\n",
    "        best_acc1 = acc\n",
    "        best_value = c\n",
    "        best_clf_knn = clf\n",
    "        \n",
    "print(best_acc1)\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_clf_knn.predict(X_test)\n",
    "print(evaluate(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, train_features, train_labels, test_features=None, test_labels=None):\n",
    "        self.train_features = train_features\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "        self.test_features = test_features\n",
    "        self.test_labels = test_labels\n",
    "        self.test_predictions = None\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, test_features):\n",
    "        pass\n",
    "\n",
    "def gaussian_probability(x, mean, stdev):\n",
    "    stdev += 1\n",
    "    exponent = exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "class NaiveBayes(Classifier):\n",
    "    \n",
    "    def __init__(self, train_features, train_labels, test_features=None, test_labels=None):\n",
    "        super().__init__(train_features, train_labels, test_features, test_labels)\n",
    "        self.label_feature_summaries = dict()\n",
    "        self.label_dictionary = dict()\n",
    "\n",
    "    def create_label_dictionary(self):\n",
    "        for label, features in zip(self.train_labels, self.train_features):\n",
    "            if label not in self.label_dictionary:\n",
    "                self.label_dictionary[label] = []\n",
    "            self.label_dictionary[label] += [features]\n",
    "\n",
    "    def summarize_dataset(self, label, documents):\n",
    "        summaries = [(np.mean(column), np.std(column), len(column)) for column in zip(*documents)]\n",
    "        self.label_feature_summaries[label] = summaries\n",
    "\n",
    "    def create_label_summaries(self):\n",
    "        for label in self.label_dictionary:\n",
    "            self.summarize_dataset(label, self.label_dictionary[label])\n",
    "\n",
    "    def train(self):\n",
    "        self.create_label_dictionary()\n",
    "        self.create_label_summaries()\n",
    "        return\n",
    "\n",
    "    def predict_one(self, ind, test):\n",
    "        total_doc_count = len(self.train_features)\n",
    "        class_probabilities = dict()\n",
    "        for label in self.label_feature_summaries:\n",
    "            label_summary = self.label_feature_summaries[label]\n",
    "            class_probabilities[label] = log(label_summary[0][-1] / total_doc_count)\n",
    "            for feature, feature_summary in zip(test, label_summary):\n",
    "                mean, std, _ = feature_summary\n",
    "                class_probabilities[label] += log(gaussian_probability(feature, mean, std))\n",
    "        self.test_predictions[ind] = max(class_probabilities.items(), key=itemgetter(1))[0]\n",
    "\n",
    "    def predict(self, test_features=None):\n",
    "        if self.test_features is None:\n",
    "            self.test_features = test_features\n",
    "        self.test_predictions = [None for _ in self.test_features]\n",
    "        for i in range(len(self.test_features)):\n",
    "            self.predict_one(i, self.test_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes2:\n",
    "    \n",
    "    def __init__(self, laplace_smooth=0, c=0):\n",
    "        self.laplace_smooth = laplace_smooth\n",
    "        self.c = c\n",
    "        self.p_f_x = []\n",
    "        self.p_y = []\n",
    "        self.num_class = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "    \n",
    "        self.num_class = len(list(set(y)))\n",
    "        numbers = [[] for _ in range(self.num_class)]\n",
    "        for x, z in zip(X, y):\n",
    "            numbers[z] += [list(x)]\n",
    "\n",
    "        self.p_y = []\n",
    "        for x in numbers:\n",
    "            self.p_y += [(len(x) + self.laplace_smooth) / (len(y) + len(numbers) * self.laplace_smooth)]\n",
    "\n",
    "        self.p_f_x = [[self.laplace_smooth] * len(X[0]) for _ in range(self.num_class)]\n",
    "        for i in range(self.num_class):\n",
    "            for j in range(len(X[0])):\n",
    "                for x in numbers[i]:\n",
    "                    if x[j] > self.c:\n",
    "                        self.p_f_x[i][j] += 1\n",
    "                self.p_f_x[i][j] /= len(numbers[i])\n",
    "\n",
    "    def predict(self, test):\n",
    "    \n",
    "        predict = []\n",
    "        for x in test:\n",
    "            res = []\n",
    "            for i in range(self.num_class):\n",
    "                p = self.p_y[i]\n",
    "                for j in range(len(x)):\n",
    "                    if x[j] > self.c:\n",
    "                        p *= self.p_f_x[i][j]\n",
    "                    else:\n",
    "                        p *= 1 - self.p_f_x[i][j]\n",
    "                res += [p]\n",
    "            predict += [res]\n",
    "\n",
    "        predict = np.array(predict)\n",
    "        predict = np.argmax(predict, axis=1)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [0.1, 0.3, 0.5]\n",
    "\n",
    "best_acc2 = 0.0\n",
    "best_value = 0\n",
    "best_clf_naive_bayes = None\n",
    "\n",
    "for l in L:\n",
    "    \n",
    "    clf = NaiveBayes2(laplace_smooth=l, c=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_validation)\n",
    "    acc = evaluate(y_validation, predicted)[\"Accuracy\"]\n",
    "    \n",
    "    if acc >= best_acc2:\n",
    "        best_acc2 = acc\n",
    "        best_value = c\n",
    "        best_clf_naive_bayes = clf\n",
    "        \n",
    "print(best_acc2)\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_clf_naive_bayes.predict(X_test)\n",
    "print(evaluate(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_List = [1/2, 1, 3/2, 2]‬\n",
    "\n",
    "best_acc3 = 0.0\n",
    "best_value = 0\n",
    "best_clf_svc = None\n",
    "\n",
    "\n",
    "for c in C_LIST:\n",
    "    \n",
    "    clf = svm.SVC(C=c)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_validation)\n",
    "    acc = evaluate(y_validation, predicted)[\"Accuracy\"]\n",
    "    \n",
    "    if acc >= best_acc3:\n",
    "        best_acc3 = acc\n",
    "        best_value = c\n",
    "        best_clf_svc = clf\n",
    "        \n",
    "print(best_acc3)\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_clf_svc.predict(X_test)\n",
    "print(evaluate(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Estimators = [50, 100, 200, 500]\n",
    "\n",
    "best_acc4 = 0.0\n",
    "best_value = 0\n",
    "best_clf_random_forest = None\n",
    "\n",
    "for n in N_Estimators:\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=n)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_validation)\n",
    "    acc = evaluate(y_validation, predicted)[\"Accuracy\"]\n",
    "    \n",
    "    if acc >= best_acc4:\n",
    "        best_acc4 = acc\n",
    "        best_value = c\n",
    "        best_clf_random_forest = clf\n",
    "        \n",
    "print(best_acc4)\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = best_clf_random_forest.predict(X_test)\n",
    "print(evaluate(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = [best_acc1, best_acc2, best_acc3, best_acc4]\n",
    "best_index = np.argmax(accs)\n",
    "print(accs[best_index])\n",
    "\n",
    "best_clf = None\n",
    "if best_index == 0:\n",
    "    best_clf = best_clf_knn\n",
    "elif best_index == 1:\n",
    "    best_clf = best_clf_naive_bayes\n",
    "elif best_index == 1:\n",
    "    best_clf = best_clf_svc\n",
    "elif best_index == 1:\n",
    "    best_clf = best_clf_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_phase1 = best_clf.predict(X_phase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringSearch(string, X, y, words, index=0, ktop=20):\n",
    "    \n",
    "    string_split = string.split(\" \")\n",
    "    words_dic_tfidfs = {x:string_split.count(x) for x in string_split}\n",
    "                                       \n",
    "    scores = {}\n",
    "    for i in range(X):\n",
    "        s = 0\n",
    "        for y in words_dic_norm:\n",
    "            try:\n",
    "                s += words_dic_tfids[y] * X[i][words.index(y)]\n",
    "            except ValueError:\n",
    "                continue\n",
    "        scores[i] = s\n",
    "    \n",
    "    if index == 0:\n",
    "        final = sorted(scores, key=scores.get, reverse=True)[:ktop]\n",
    "    else:\n",
    "        semi_final = sorted(scores, key=scores.get, reverse=True)\n",
    "        final = []\n",
    "        for x in semi_final:\n",
    "            if y[x] == index:\n",
    "                final += [x]\n",
    "        final = final[:ktop]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    print(\"Please enter 0 if you do not want to use view, 1 and -1 if you want to use, and 404 if you want to exit!\")\n",
    "    \n",
    "    s = input()\n",
    "    \n",
    "    if s == '0':\n",
    "        print(\"Please enter your text.\")\n",
    "        string = input()\n",
    "        \n",
    "        output = stringSearch(string, X_phase1, y_phase1, words, index=0)\n",
    "        print(output)\n",
    "        \n",
    "    elif s == '1':\n",
    "        print(\"Please enter your text.\")\n",
    "        string = input()\n",
    "        \n",
    "        output = stringSearch(string, X_phase1, y_phase1, words, index=1)\n",
    "        print(output)\n",
    "        \n",
    "    elif s == '-1':\n",
    "        print(\"Please enter your text.\")\n",
    "        string = input()\n",
    "        \n",
    "        output = stringSearch(string, X_phase1, y_phase1, words, index=-1)\n",
    "        print(output)\n",
    "        \n",
    "    elif s == '404':\n",
    "        print(\"END.\")\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        print(\"Please enter a number according to the guideline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
