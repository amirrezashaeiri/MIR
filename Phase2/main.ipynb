{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "import ast\n",
    "\n",
    "from math import exp, sqrt, pi, log\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_english(data):\n",
    "\n",
    "    data_desc_title = data[['description', 'title']]\n",
    "    data_desc_title_ls = data_desc_title.values.tolist()\n",
    "\n",
    "    tokenized = []\n",
    "    for data in data_desc_title_ls:\n",
    "        tokenized_desc = word_tokenize(data[0].lower())\n",
    "        tokenized_title = word_tokenize(data[1].lower())\n",
    "        tokenized.append([tokenized_desc, tokenized_title])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokenized_lemmatized = []\n",
    "\n",
    "    for data in tokenized:\n",
    "\n",
    "        lemmatized_tokenized_desc = []\n",
    "        for word in data[0]:\n",
    "            lemmatized_tokenized_desc.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "        lemmatized_tokenized_desc = [word for word in lemmatized_tokenized_desc if word.isalnum()]\n",
    "\n",
    "        lemmatized_tokenized_title = []\n",
    "        for word in data[1]:\n",
    "            lemmatized_tokenized_title.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "        lemmatized_tokenized_title = [word for word in lemmatized_tokenized_title if word.isalnum()]\n",
    "\n",
    "        tokenized_lemmatized.append([lemmatized_tokenized_desc, lemmatized_tokenized_title])\n",
    "        \n",
    "\n",
    "    return tokenized_lemmatized\n",
    "\n",
    "\n",
    "def remove_stop_words_english(tokenized_lemmatized):\n",
    "\n",
    "\n",
    "    list_of_words_frequency = []\n",
    "    \n",
    "    for data in tokenized_lemmatized:\n",
    "        \n",
    "        for word in data[0]:\n",
    "            if word not in [i[0] for i in list_of_words_frequency]:\n",
    "                list_of_words_frequency.append([word, 1])\n",
    "            else:\n",
    "                list_of_words_frequency[[i[0] for i in list_of_words_frequency].index(word)][1] += 1\n",
    "\n",
    "        for word in data[1]:\n",
    "            if word not in [i[0] for i in list_of_words_frequency]:\n",
    "                list_of_words_frequency.append([word, 1])\n",
    "            else:\n",
    "                list_of_words_frequency[[i[0] for i in list_of_words_frequency].index(word)][1] += 1\n",
    "\n",
    "    list_of_words_frequency = sorted(list_of_words_frequency, key=lambda l: l[1], reverse=True)\n",
    "\n",
    "\n",
    "    number_of_stop_words = 10\n",
    "    stop_words = [i[0] for i in list_of_words_frequency][:number_of_stop_words]\n",
    "    with open('data/stop_words_english.txt', 'w', encoding='utf-8') as f:\n",
    "        for page in stop_words:\n",
    "            f.write(\"%s\\n\" % page)\n",
    "    \n",
    "    \n",
    "    tokenized_lemmatized_removed_stop_words = []\n",
    "    for data in tokenized_lemmatized:\n",
    "        lemmatized_tokenized_removed_stop_words_desc = [word for word in data[0] if word not in stop_words]\n",
    "        lemmatized_tokenized_removed_stop_words_title = [word for word in data[1] if word not in stop_words]\n",
    "        tokenized_lemmatized_removed_stop_words.append(\n",
    "            [lemmatized_tokenized_removed_stop_words_desc, lemmatized_tokenized_removed_stop_words_title])\n",
    "\n",
    "        \n",
    "    return tokenized_lemmatized_removed_stop_words\n",
    "\n",
    "\n",
    "def add_id_english(tokenized_lemmatized_removed_stop_words):\n",
    "\n",
    "    merged_id_english = []\n",
    "\n",
    "    for i in range(len(tokenized_lemmatized_removed_stop_words)):\n",
    "        merged_id_english.append(\n",
    "            [i + 1, tokenized_lemmatized_removed_stop_words[i][0], tokenized_lemmatized_removed_stop_words[i][1]])\n",
    "\n",
    "    return merged_id_english\n",
    "\n",
    "\n",
    "def preProcess(data,words):\n",
    "    \n",
    "    data_desc_title = data[['description', 'title']]\n",
    "    data_desc_title_ls = data_desc_title.values.tolist()\n",
    "    tokenized_lemmatized = prepare_text_english(data)\n",
    "    tokenized_lemmatized_removed_stop_words = remove_stop_words_english(tokenized_lemmatized)\n",
    "    merged_id_english = add_id_english(tokenized_lemmatized_removed_stop_words)\n",
    "\n",
    "def preProcess(data,words):\n",
    "    \n",
    "    data_desc_title = data[['description', 'title']]\n",
    "    data_desc_title_ls = data_desc_title.values.tolist()\n",
    "\n",
    "    if words is None:\n",
    "        \n",
    "        tokenized_lemmatized = prepare_text_english(data)\n",
    "        tokenized_lemmatized_removed_stop_words = remove_stop_words_english(tokenized_lemmatized)\n",
    "        merged_id_english = add_id_english(tokenized_lemmatized_removed_stop_words)\n",
    "\n",
    "        arr=[]\n",
    "        for i in range(len(merged_id_english)):\n",
    "            dic = {}\n",
    "            dic['text']=merged_id_english[i][1]\n",
    "            dic['title'] =merged_id_english[i][2]\n",
    "            arr.append(dic)\n",
    "        corpes = []\n",
    "\n",
    "        for i in tokenized_lemmatized_removed_stop_words:\n",
    "            corpes.extend(i[0])\n",
    "            corpes.extend(i[1])\n",
    "            \n",
    "        corpes = list(set(corpes))\n",
    "\n",
    "        return arr, corpes\n",
    "\n",
    "    else:\n",
    "\n",
    "        arr = []\n",
    "        for i in data_desc_title_ls:\n",
    "            \n",
    "            dic = {}\n",
    "            decs = []\n",
    "            for j in i[0].split():\n",
    "                if j in words:\n",
    "                    decs.append(j)\n",
    "\n",
    "            titles = []\n",
    "            for j in i[1].split():\n",
    "                if j in words:\n",
    "                    titles.append(j)\n",
    "                    \n",
    "            dic[\"text\"] = decs\n",
    "            dic[\"title\"] = titles\n",
    "            arr.append(dic)\n",
    "\n",
    "        return arr, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_X(raw_data, words=None, idfs=None, coeff=2):\n",
    "    \n",
    "    main_data, words = preProcess(raw_data, words)\n",
    "    \n",
    "    n_doc = len(main_data)\n",
    "    n_word = len(words)\n",
    "    \n",
    "    vector_space_title = np.zeros((n_doc, n_word))\n",
    "    vector_space_text = np.zeros((n_doc, n_word))\n",
    "\n",
    "    for i in range(n_doc):\n",
    "        for y in main_data[i]['title']:\n",
    "            vector_space_title[i, words.index(y)] += 1\n",
    "        for y in main_data[i]['text']:\n",
    "            vector_space_text[i, words.index(y)] += 1\n",
    "    \n",
    "    vector_space_title_tfs = vector_space_title\n",
    "    vector_space_text_tfs = vector_space_text\n",
    "    \n",
    "    vector_space_title_idfs = []\n",
    "    vector_space_text_idfs = []\n",
    "    if idfs is None:\n",
    "        vector_space_title_idfs = np.log10(n_word / (np.count_nonzero(vector_space_title, axis=0) + 1))\n",
    "        vector_space_text_idfs = np.log10(n_word / (np.count_nonzero(vector_space_text, axis=0) + 1))\n",
    "    else:\n",
    "        vector_space_title_idfs = idfs[0]\n",
    "        vector_space_text_idfs = idfs[1]\n",
    "    \n",
    "    vector_space_title_tfidfs = np.multiply(vector_space_title_tfs, vector_space_title_idfs)\n",
    "    vector_space_text_tfidfs = np.multiply(vector_space_text_tfs, vector_space_text_idfs)\n",
    "    \n",
    "    final_vector_space = coeff * vector_space_title_tfidfs + vector_space_text_tfidfs\n",
    "    \n",
    "    return final_vector_space, words, (vector_space_title_idfs, vector_space_text_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_phase2 = pd.read_csv(\"./data/train.csv\")\n",
    "test_data_phase2 = pd.read_csv(\"./data/test.csv\")\n",
    "data_phase1 = pd.read_csv(\"./data/ted_talks.csv\")\n",
    "\n",
    "X_train, words, idfs = make_X(train_data_phase2)\n",
    "X_test, _, _ = make_X(test_data_phase2, words, idfs=idfs)\n",
    "X_phase1, _, _ = make_X(data_phase1, words)\n",
    "\n",
    "y_train = train_data_phase2[\"views\"].values\n",
    "y_test = test_data_phase2[\"views\"].values\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_predicted, epsilon=10**-9):\n",
    "    \n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    for y1, y2 in zip(y_true, y_predicted):\n",
    "        if y1 == y2:\n",
    "            if y1 == 1:\n",
    "                TP += 1\n",
    "            elif y2 == -1:\n",
    "                TN += 1\n",
    "        elif y1 != y2:\n",
    "            if y1 == 1:\n",
    "                FN += 1\n",
    "            elif y1 == -1:\n",
    "                FP += 1\n",
    "    \n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP + epsilon)\n",
    "    recall = TP / (TP + FN + epsilon)\n",
    "    specificity = TN / (TN + FP + epsilon)\n",
    "    f1 = (2 * precision * recall) / (precision + recall + epsilon)\n",
    "    \n",
    "    dic = {\"Precision\": precision, \"Recall\": recall, \"Accuracy\": accuracy, \"F1_Score\": f1,\n",
    "           \"Sensitivity\": recall, \"Specificity\": specificity}\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidianDistance(sample1, sample2):\n",
    "    dis = np.linalg.norm(sample1 - sample2)\n",
    "    return dis\n",
    "\n",
    "def euclidianDistance2(sample1, sample2):\n",
    "    distance = 0\n",
    "    for i in range(len(sample1)):\n",
    "        distance += pow(sample1[i] - sample2[i], 2)\n",
    "    distance = math.sqrt(distance)\n",
    "    return distance\n",
    "\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, k, dis):\n",
    "        self.k = k\n",
    "        self.dis = dis\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "\n",
    "    def distance(self, sample1, sample2, distance_type):\n",
    "        if distance_type == 'euclidian':\n",
    "            return euclidianDistance(sample1, sample2)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, test):\n",
    "\n",
    "        predict = []\n",
    "        \n",
    "        for x in test:\n",
    "\n",
    "            temp = []\n",
    "            for y in self.X:\n",
    "                temp += [self.distance(x, y, self.dis)]\n",
    "\n",
    "            temp = list(zip(temp, self.y))\n",
    "            temp = sorted(temp, key=lambda a_entry: a_entry[0])\n",
    "            temp = np.array(temp)\n",
    "            temp = temp[:self.k, 1]\n",
    "            temp = np.unique(temp, return_counts=True)\n",
    "            predict += [temp[0][temp[1].argmax()]]\n",
    "        \n",
    "        return np.array(predict).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5623188405797102\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "K = [1, 5, 9]\n",
    "\n",
    "best_acc1 = 0.0\n",
    "best_value = 0\n",
    "best_clf_knn = None\n",
    "\n",
    "for k in K:\n",
    "    \n",
    "    clf = KNN(k=k, dis='euclidian')\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_validation)\n",
    "    acc = evaluate(y_validation, predicted)[\"Accuracy\"]\n",
    "    \n",
    "    if acc >= best_acc1:\n",
    "        best_acc1 = acc\n",
    "        best_value = k\n",
    "        best_clf_knn = clf\n",
    "        \n",
    "print(best_acc1)\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.6363636363347107, 'Recall': 0.11570247933788676, 'Accuracy': 0.5490196078431373, 'F1_Score': 0.19580419554110226, 'Sensitivity': 0.11570247933788676, 'Specificity': 0.9402985074556695}\n"
     ]
    }
   ],
   "source": [
    "predicted = best_clf_knn.predict(X_test)\n",
    "print(evaluate(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, train_features, train_labels, test_features=None, test_labels=None):\n",
    "        self.train_features = train_features\n",
    "        self.train_labels = train_labels\n",
    "\n",
    "        self.test_features = test_features\n",
    "        self.test_labels = test_labels\n",
    "        self.test_predictions = None\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, test_features):\n",
    "        pass\n",
    "\n",
    "def gaussian_probability(x, mean, stdev):\n",
    "    stdev += 1\n",
    "    exponent = exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
    "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    "\n",
    "class NaiveBayes(Classifier):\n",
    "    \n",
    "    def __init__(self, train_features, train_labels, test_features=None, test_labels=None):\n",
    "        super().__init__(train_features, train_labels, test_features, test_labels)\n",
    "        self.label_feature_summaries = dict()\n",
    "        self.label_dictionary = dict()\n",
    "\n",
    "    def create_label_dictionary(self):\n",
    "        for label, features in zip(self.train_labels, self.train_features):\n",
    "            if label not in self.label_dictionary:\n",
    "                self.label_dictionary[label] = []\n",
    "            self.label_dictionary[label] += [features]\n",
    "\n",
    "    def summarize_dataset(self, label, documents):\n",
    "        summaries = [(np.mean(column), np.std(column), len(column)) for column in zip(*documents)]\n",
    "        self.label_feature_summaries[label] = summaries\n",
    "\n",
    "    def create_label_summaries(self):\n",
    "        for label in self.label_dictionary:\n",
    "            self.summarize_dataset(label, self.label_dictionary[label])\n",
    "\n",
    "    def train(self):\n",
    "        self.create_label_dictionary()\n",
    "        self.create_label_summaries()\n",
    "        return\n",
    "\n",
    "    def predict_one(self, ind, test):\n",
    "        total_doc_count = len(self.train_features)\n",
    "        class_probabilities = dict()\n",
    "        for label in self.label_feature_summaries:\n",
    "            label_summary = self.label_feature_summaries[label]\n",
    "            class_probabilities[label] = log(label_summary[0][-1] / total_doc_count)\n",
    "            for feature, feature_summary in zip(test, label_summary):\n",
    "                mean, std, _ = feature_summary\n",
    "                class_probabilities[label] += log(gaussian_probability(feature, mean, std))\n",
    "        self.test_predictions[ind] = max(class_probabilities.items(), key=itemgetter(1))[0]\n",
    "\n",
    "    def predict(self, test_features=None):\n",
    "        if self.test_features is None:\n",
    "            self.test_features = test_features\n",
    "        self.test_predictions = [None for _ in self.test_features]\n",
    "        for i in range(len(self.test_features)):\n",
    "            self.predict_one(i, self.test_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = NaiveBayes(X_train, y_train, X_validation, y_validation)\n",
    "NB.train()\n",
    "NB.predict()\n",
    "\n",
    "best_acc2 = evaluate(y_test, NB.test_predictions)[\"Accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.5666666666572222, 'Recall': 0.28099173553486784, 'Accuracy': 0.5568627450980392, 'F1_Score': 0.37569060728744547, 'Sensitivity': 0.28099173553486784, 'Specificity': 0.8059701492477167}\n"
     ]
    }
   ],
   "source": [
    "NB = NaiveBayes(X_train, y_train, X_test, y_test)\n",
    "NB.train()\n",
    "NB.predict()\n",
    "\n",
    "print(evaluate(y_test, NB.test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NaiveBayes2:\n",
    "    \n",
    "#     def __init__(self, laplace_smooth=0, c=0):\n",
    "#         self.laplace_smooth = laplace_smooth\n",
    "#         self.c = c\n",
    "#         self.p_f_x = []\n",
    "#         self.p_y = []\n",
    "#         self.num_class = 0\n",
    "    \n",
    "#     def fit(self, X, y):\n",
    "    \n",
    "#         self.num_class = len(list(set(y)))\n",
    "#         numbers = [[] for _ in range(self.num_class)]\n",
    "#         for x, z in zip(X, y):\n",
    "#             numbers[z] += [list(x)]\n",
    "\n",
    "#         self.p_y = []\n",
    "#         for x in numbers:\n",
    "#             self.p_y += [(len(x) + self.laplace_smooth) / (len(y) + len(numbers) * self.laplace_smooth)]\n",
    "\n",
    "#         self.p_f_x = [[self.laplace_smooth] * len(X[0]) for _ in range(self.num_class)]\n",
    "#         for i in range(self.num_class):\n",
    "#             for j in range(len(X[0])):\n",
    "#                 for x in numbers[i]:\n",
    "#                     if x[j] > self.c:\n",
    "#                         self.p_f_x[i][j] += 1\n",
    "#                 self.p_f_x[i][j] /= len(numbers[i])\n",
    "\n",
    "#     def predict(self, test):\n",
    "    \n",
    "#         predict = []\n",
    "#         for x in test:\n",
    "#             res = []\n",
    "#             for i in range(self.num_class):\n",
    "#                 p = self.p_y[i]\n",
    "#                 for j in range(len(x)):\n",
    "#                     if x[j] > self.c:\n",
    "#                         p *= self.p_f_x[i][j]\n",
    "#                     else:\n",
    "#                         p *= 1 - self.p_f_x[i][j]\n",
    "#                 res += [p]\n",
    "#             predict += [res]\n",
    "\n",
    "#         predict = np.array(predict)\n",
    "#         predict = np.argmax(predict, axis=1)\n",
    "#         return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = [0.1, 0.3, 0.5]\n",
    "\n",
    "# best_acc2 = 0.0\n",
    "# best_value = 0\n",
    "# best_clf_naive_bayes = None\n",
    "\n",
    "# for l in L:\n",
    "    \n",
    "#     clf = NaiveBayes2(laplace_smooth=l, c=0.1)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     predicted = clf.predict(X_validation)\n",
    "#     acc = evaluate(y_validation, predicted)[\"Accuracy\"]\n",
    "    \n",
    "#     if acc >= best_acc2:\n",
    "#         best_acc2 = acc\n",
    "#         best_value = l\n",
    "#         best_clf_naive_bayes = clf\n",
    "        \n",
    "# print(best_acc2)\n",
    "# print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = best_clf_naive_bayes.predict(X_test)\n",
    "# print(evaluate(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6086956521739131\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "C_List = [1/2, 1, 3/2, 2]\n",
    "\n",
    "best_acc3 = 0.0\n",
    "best_value = 0\n",
    "best_clf_svc = None\n",
    "\n",
    "\n",
    "for c in C_List:\n",
    "    \n",
    "    clf = svm.SVC(C=c, kernel=\"rbf\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_validation)\n",
    "    acc = evaluate(y_validation, predicted)[\"Accuracy\"]\n",
    "    \n",
    "    if acc >= best_acc3:\n",
    "        best_acc3 = acc\n",
    "        best_value = c\n",
    "        best_clf_svc = clf\n",
    "        \n",
    "print(best_acc3)\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.7551020408009164, 'Recall': 0.305785123964415, 'Accuracy': 0.6235294117647059, 'F1_Score': 0.4352941172316263, 'Sensitivity': 0.305785123964415, 'Specificity': 0.9104477611872356}\n"
     ]
    }
   ],
   "source": [
    "predicted = best_clf_svc.predict(X_test)\n",
    "print(evaluate(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6086956521739131\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "N_Estimators = [50, 100, 200, 500]\n",
    "\n",
    "best_acc4 = 0.0\n",
    "best_value = 0\n",
    "best_clf_random_forest = None\n",
    "\n",
    "for n in N_Estimators:\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=n)\n",
    "    clf.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_validation)\n",
    "    acc = evaluate(y_validation, predicted)[\"Accuracy\"]\n",
    "    \n",
    "    if acc >= best_acc4:\n",
    "        best_acc4 = acc\n",
    "        best_value = n\n",
    "        best_clf_random_forest = clf\n",
    "        \n",
    "print(best_acc4)\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.7674418604472688, 'Recall': 0.2727272727250188, 'Accuracy': 0.615686274509804, 'F1_Score': 0.4024390239984384, 'Sensitivity': 0.2727272727250188, 'Specificity': 0.9253731343214525}\n"
     ]
    }
   ],
   "source": [
    "predicted = best_clf_random_forest.predict(X_test)\n",
    "print(evaluate(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6086956521739131\n",
      "SVC\n"
     ]
    }
   ],
   "source": [
    "accs = [best_acc1, best_acc2, best_acc3, best_acc4]\n",
    "best_index = np.argmax(accs)\n",
    "print(accs[best_index])\n",
    "\n",
    "best_clf = None\n",
    "if best_index == 0:\n",
    "    best_clf = best_clf_knn\n",
    "    print(\"KNN\")\n",
    "elif best_index == 1:\n",
    "    best_clf = best_clf_naive_bayes\n",
    "    print(\"Naive Bayes\")\n",
    "elif best_index == 2:\n",
    "    best_clf = best_clf_svc\n",
    "    print(\"SVC\")\n",
    "elif best_index == 3:\n",
    "    best_clf = best_clf_random_forest\n",
    "    print(\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_phase1 = best_clf.predict(X_phase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_preProcess_english(str):\n",
    "    \n",
    "    tokenized_str = word_tokenize(str.lower())\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized_tokenized = []\n",
    "    for word in tokenized_str:\n",
    "        lemmatized_tokenized.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "    lemmatized_tokenized = [word for word in lemmatized_tokenized if word.isalnum()]\n",
    "    with open('data/stop_words_english.txt', encoding='utf-8') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    stop_words = lines\n",
    "\n",
    "    tokenized_lemmatized_removed_stop_words_str = [word for word in lemmatized_tokenized if\n",
    "                                                   word not in stop_words]\n",
    "    \n",
    "    return tokenized_lemmatized_removed_stop_words_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringSearch(string, X, Y, words, index=0, ktop=20):\n",
    "    \n",
    "    string_split = string_preProcess_english(string)\n",
    "    print(string_split)\n",
    "    words_dic_tfidfs = {x:string_split.count(x) for x in string_split}\n",
    "                                       \n",
    "    scores = {}\n",
    "    for i in range(len(X)):\n",
    "        s = 0\n",
    "        for y in words_dic_tfidfs:\n",
    "            try:\n",
    "                s += words_dic_tfidfs[y] * X[i][words.index(y)]\n",
    "            except ValueError:\n",
    "                continue\n",
    "        scores[i] = s\n",
    "    \n",
    "    if index == 0:\n",
    "        final = sorted(scores, key=scores.get, reverse=True)[:ktop]\n",
    "    else:\n",
    "        semi_final = sorted(scores, key=scores.get, reverse=True)\n",
    "        final = []\n",
    "        for x in semi_final:\n",
    "            if Y[x] == index:\n",
    "                final += [x]\n",
    "        final = final[:ktop]\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter 0 if you do not want to use view, 1 and -1 if you want to use, and 404 if you want to exit!\n",
      "0\n",
      "Please enter your text.\n",
      "car machine computer\n",
      "['car', 'machine', 'computer']\n",
      "[2004, 948, 240, 1730, 2121, 901, 1539, 363, 1283, 270, 750, 893, 1130, 1293, 1349, 1655, 1693, 1962, 2155, 1941]\n",
      "\n",
      "Please enter 0 if you do not want to use view, 1 and -1 if you want to use, and 404 if you want to exit!\n",
      "1\n",
      "Please enter your text.\n",
      "car machine computer\n",
      "['car', 'machine', 'computer']\n",
      "[2004, 1730, 1539, 893, 1655, 1962, 2155, 1560, 2110, 2236, 2318, 2503, 2139, 2153, 13, 812, 984, 1028, 1424, 1551]\n",
      "\n",
      "Please enter 0 if you do not want to use view, 1 and -1 if you want to use, and 404 if you want to exit!\n",
      "-1\n",
      "Please enter your text.\n",
      "car machine computer\n",
      "['car', 'machine', 'computer']\n",
      "[948, 240, 2121, 901, 363, 1283, 270, 750, 1130, 1293, 1349, 1693, 1941, 2502, 227, 598, 702, 1826, 2401, 2460]\n",
      "\n",
      "Please enter 0 if you do not want to use view, 1 and -1 if you want to use, and 404 if you want to exit!\n",
      "404\n",
      "END.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    print(\"Please enter 0 if you do not want to use view, 1 and -1 if you want to use, and 404 if you want to exit!\")\n",
    "    \n",
    "    s = input()\n",
    "    \n",
    "    if s == '0':\n",
    "        print(\"Please enter your text.\")\n",
    "        string = input()\n",
    "        \n",
    "        output = stringSearch(string, X_phase1, y_phase1, words, index=0)\n",
    "        print(output)\n",
    "        print()\n",
    "        \n",
    "    elif s == '1':\n",
    "        print(\"Please enter your text.\")\n",
    "        string = input()\n",
    "        \n",
    "        output = stringSearch(string, X_phase1, y_phase1, words, index=1)\n",
    "        print(output)\n",
    "        print()\n",
    "        \n",
    "    elif s == '-1':\n",
    "        print(\"Please enter your text.\")\n",
    "        string = input()\n",
    "        \n",
    "        output = stringSearch(string, X_phase1, y_phase1, words, index=-1)\n",
    "        print(output)\n",
    "        print()\n",
    "        \n",
    "    elif s == '404':\n",
    "        print(\"END.\")\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        print(\"Please enter a number according to the guideline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1000)\n",
    "X_train2 = pca.fit_transform(X_train)\n",
    "X_validation2 = pca.transform(X_validation)\n",
    "X_test2 = pca.transform(X_test)\n",
    "X_pahse1_2 = pca.transform(X_phase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5971014492753624\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "C_List = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "best_acc5 = 0.0\n",
    "best_value = 0\n",
    "best_clf_svc2 = None\n",
    "\n",
    "\n",
    "for c in C_List:\n",
    "    \n",
    "    clf = svm.SVC(C=c, kernel=\"rbf\")\n",
    "    clf.fit(X_train2, y_train)\n",
    "    predicted = clf.predict(X_validation2)\n",
    "    acc = evaluate(y_validation, predicted)[\"Accuracy\"]\n",
    "    \n",
    "    if acc >= best_acc5:\n",
    "        best_acc5 = acc\n",
    "        best_value = c\n",
    "        best_clf_svc2 = clf\n",
    "        \n",
    "print(best_acc5)\n",
    "print(best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 0.7878787878549128, 'Recall': 0.2148760330560754, 'Accuracy': 0.6, 'F1_Score': 0.3376623373212177, 'Sensitivity': 0.2148760330560754, 'Specificity': 0.947761194022778}\n"
     ]
    }
   ],
   "source": [
    "predicted = best_clf_svc2.predict(X_test2)\n",
    "print(evaluate(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
